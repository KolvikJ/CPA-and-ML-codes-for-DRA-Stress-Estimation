{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c4311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# ----------------------------\n",
    "# User-defined Configuration\n",
    "# ----------------------------\n",
    "\n",
    "# Name of the CSV file containing training data\n",
    "DATA_FILE = \"ExampleFilename\"    #Do not include \".csv\"\n",
    "\n",
    "# Choose the ratio of data which is to be used for training and validation \n",
    "# Recommended: 80% training and 20% validating\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.2\n",
    "\n",
    "# Insert the desired name of your trained model to be stored as\n",
    "model_name = \"ExampleModelName\"    #Do not include \".pkl\"\n",
    "\n",
    "# (Optional) random seed for reproducibility\n",
    "# For truly random shuffling input: None\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# ----------------------------\n",
    "\n",
    "# Checking that ratios sum to 1.\n",
    "assert abs(TRAIN_RATIO + VALID_RATIO - 1.0) < 1e-6, \"Training and validation ratios must sum to 1.\"\n",
    "\n",
    "#Reads the datafile\n",
    "data = pd.read_csv(f\"{DATA_FILE}.csv\")\n",
    "data = data.ffill()\n",
    "\n",
    "# Shuffles the dataset.\n",
    "data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Defining predictors and target for our ANN.\n",
    "PREDICTORS = [f\"xy{i}\" for i in range(1, 151)]\n",
    "TARGET = \"preLoad\"\n",
    "\n",
    "# Scaling the predictors.\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "# Split data into training and validation sets.\n",
    "train_end = int(TRAIN_RATIO * len(data))\n",
    "train_df = data.iloc[:train_end]\n",
    "valid_df = data.iloc[train_end:]\n",
    "\n",
    "train_x, train_y = train_df[PREDICTORS].to_numpy(), train_df[[TARGET]].to_numpy()\n",
    "valid_x, valid_y = valid_df[PREDICTORS].to_numpy(), valid_df[[TARGET]].to_numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Functions for ANN and Grid Search\n",
    "# ----------------------------\n",
    "\n",
    "# ANN structure\n",
    "layer_config = [150, 10, 10, 1]\n",
    "\n",
    "def init_layers(layer_config):\n",
    "    \"\"\"Initialize ANN layers with random weights and biases.\"\"\"\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_config)):\n",
    "        weights = np.random.rand(layer_config[i-1], layer_config[i]) / 5 - 0.1\n",
    "        biases = np.ones((1, layer_config[i]))\n",
    "        layers.append([weights, biases])\n",
    "    return layers\n",
    "\n",
    "def forward(batch, layers):\n",
    "    \"\"\"Forward propagation through the network.\"\"\"\n",
    "    hiddens = [batch.copy()]\n",
    "    for i in range(len(layers)):\n",
    "        batch = np.matmul(batch, layers[i][0]) + layers[i][1]\n",
    "        # Use ReLU activation in hidden layers.\n",
    "        if i < len(layers) - 1:\n",
    "            batch = np.maximum(batch, 0)\n",
    "        hiddens.append(batch.copy())\n",
    "    return batch, hiddens\n",
    "\n",
    "def backward(layers, hiddens, grad, lr):\n",
    "    \"\"\"Backward propagation through the network.\"\"\"\n",
    "    for i in range(len(layers) - 1, -1, -1):\n",
    "        if i != len(layers) - 1:\n",
    "            grad = np.multiply(grad, np.heaviside(hiddens[i+1], 0))  # ReLU derivative.\n",
    "        w_grad = hiddens[i].T @ grad\n",
    "        b_grad = np.mean(grad, axis=0)\n",
    "        layers[i][0] -= lr * w_grad\n",
    "        layers[i][1] -= lr * b_grad\n",
    "        grad = grad @ layers[i][0].T\n",
    "    return layers\n",
    "\n",
    "def mse(actual, predicted):\n",
    "    \"\"\"Mean Squared Error (MSE).\"\"\"\n",
    "    return (actual - predicted) ** 2\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    \"\"\"Gradient of the MSE.\"\"\"\n",
    "    return (predicted - actual)\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    \"\"\"Compute the R-squared metric.\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot if ss_tot != 0 else 1.0\n",
    "\n",
    "def regression_accuracy(actual, predicted):\n",
    "    \"\"\"Compute 'regression accuracy' as a percentage.\"\"\"\n",
    "    accs = []\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == 0 and p == 0:\n",
    "            acc = 100\n",
    "        elif a == 0:\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = (min(a, p) / max(a, p)) * 100\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)\n",
    "\n",
    "\n",
    "def train_ann(train_x, train_y, valid_x, valid_y, lr, epochs, batch_size, layer_config, best_valid_mse=float('inf'), previous_best_layers=None):\n",
    "    # Variable defined to separate Grid Search from final training\n",
    "    initial_best_valid_mse = best_valid_mse\n",
    "    \n",
    "    layers = init_layers(layer_config)\n",
    "    best_layers = None\n",
    "    best_epoch = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data each epoch.\n",
    "        perm = np.random.permutation(len(train_x))\n",
    "        train_x_shuffled = train_x[perm]\n",
    "        train_y_shuffled = train_y[perm]\n",
    "        \n",
    "        #Training the model\n",
    "        for i in range(0, len(train_x_shuffled), batch_size):\n",
    "            x_batch = train_x_shuffled[i:i+batch_size]\n",
    "            y_batch = train_y_shuffled[i:i+batch_size]\n",
    "            pred, hiddens = forward(x_batch, layers)\n",
    "            grad = mse_grad(y_batch, pred) / (2 * x_batch.shape[0])\n",
    "            layers = backward(layers, hiddens, grad, lr)\n",
    "\n",
    "        # Validation: storing the best weights for our model\n",
    "        valid_preds, _ = forward(valid_x, layers)\n",
    "        valid_mse = np.mean(mse(valid_y, valid_preds))\n",
    "        if valid_mse < best_valid_mse:\n",
    "            best_valid_mse = valid_mse\n",
    "            best_layers = [[np.copy(w), np.copy(b)] for w, b in layers]\n",
    "            best_epoch = epoch + 1  # Add 1 so it starts at \n",
    "\n",
    "            \n",
    "            \n",
    "    # Info print when function is used in final training\n",
    "    if initial_best_valid_mse < float('inf'):\n",
    "        if best_layers is None:\n",
    "            best_layers = previous_best_layers\n",
    "            print(\"No improvement was found during final training; kept previous model parameters from Grid Search.\")\n",
    "        else:\n",
    "            print(f\"Improved validation MSE found at epoch {best_epoch} with a value of {best_valid_mse:.2f}\")\n",
    "    \n",
    "    return best_layers, best_valid_mse\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Tuning Alogrithm for Hyperparameters using Grid Search\n",
    "# ----------------------------\n",
    "\n",
    "#Defined grid with hyperparameters to be tuned\n",
    "lr_list = [1e-7, 1e-6, 5e-6, 1e-5]\n",
    "batch_size_list = [8, 12, 16, 20]\n",
    "epochs_list = [1000, 3000, 5000, 10000]\n",
    "\n",
    "\n",
    "#Defining variables which store the best case (lowest Valid MSE)\n",
    "best_overall_mse = float('inf')\n",
    "best_params = {}\n",
    "best_model_layers = None\n",
    "\n",
    "print(\"Starting Grid Search for Hyperparameter Tuning...\\n\")\n",
    "for lr in lr_list:\n",
    "    for batch_size in batch_size_list:\n",
    "        for epochs in epochs_list:\n",
    "            print(f\"Testing: lr={lr}, batch_size={batch_size}, epochs={epochs}\")\n",
    "            model_layers, valid_mse = train_ann(train_x, train_y, valid_x, valid_y, lr, epochs, batch_size, layer_config)\n",
    "            print(f\"  --> Validation MSE: {valid_mse:.2f}\\n\")\n",
    "            if valid_mse < best_overall_mse:\n",
    "                best_overall_mse = valid_mse\n",
    "                best_params = {'lr': lr, 'batch_size': batch_size, 'epochs': epochs}\n",
    "                best_model_layers = model_layers\n",
    "\n",
    "print(\"Grid Search Completed.\")\n",
    "print(\"Best Hyperparameters Found:\", best_params)\n",
    "print(f\"Best Validation MSE: {best_overall_mse:.2f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Final Training with the Best Hyperparameters\n",
    "# ----------------------------\n",
    "\n",
    "# Can adjust final training epochs as desired. (Only adjusts the model if it finds an epoch with lower Valid MSE)\n",
    "final_epochs = 20000\n",
    "final_layers, _ = train_ann(train_x, train_y, valid_x, valid_y, best_params['lr'], final_epochs, best_params['batch_size'], layer_config, best_overall_mse, best_model_layers)\n",
    "\n",
    "# Evaluating final model on validation data.\n",
    "final_valid_preds, _ = forward(valid_x, final_layers)\n",
    "final_valid_mse = np.mean(mse(valid_y, final_valid_preds))\n",
    "final_r2 = compute_r2(valid_y.flatten(), final_valid_preds.flatten())\n",
    "final_reg_acc = regression_accuracy(valid_y.flatten(), final_valid_preds.flatten())\n",
    "\n",
    "# Display final validation metrics in a table.\n",
    "print(\"\\nFinal Model Evaluation on Validation Data:\")\n",
    "print(f\"Final Valid MSE: {final_valid_mse:.2f}\")\n",
    "print(f\"Final RÂ²: {final_r2:.2f}\")\n",
    "print(f\"Final Regression Accuracy: {final_reg_acc:.2f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# Save the Best Model\n",
    "# ----------------------------\n",
    "model_data = {\n",
    "    \"layers\": final_layers,\n",
    "    \"scaler\": scaler,\n",
    "    \"predictors\": PREDICTORS,\n",
    "    \"target\": TARGET\n",
    "}\n",
    "\n",
    "with open(f\"{model_name}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"\\nBest model saved to '{model_name}.pkl'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
